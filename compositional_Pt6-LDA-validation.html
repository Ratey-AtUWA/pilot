<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>LDA validation</title>

<script src="compositional_Pt6-LDA-validation_files/header-attrs-2.14/header-attrs.js"></script>
<script src="compositional_Pt6-LDA-validation_files/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="compositional_Pt6-LDA-validation_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="compositional_Pt6-LDA-validation_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="compositional_Pt6-LDA-validation_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="compositional_Pt6-LDA-validation_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="compositional_Pt6-LDA-validation_files/navigation-1.1/tabsets.js"></script>
<link href="compositional_Pt6-LDA-validation_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="compositional_Pt6-LDA-validation_files/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">



<p><img src="page%201%20header%20Pt6.png" width="100%" style="display: block; margin: auto 0 auto auto;" /></p>
<hr width="50%" align="left">
       <p>[ <a href="compositional_Pt5-Hallberg_ALR-ILR_LDA.html">Previous page</a> | <a href="https://ratey-atuwa.github.io/pilot/index.html">Return to Contents page</a> |
       <a href="Pt7-usgs_LDA-on-PCA.html"> Next page</a> ]<br>
       [ <a href="https://raw.githubusercontent.com/Ratey-AtUWA/compositional_data/main/compositional_Pt6-LDA%20validation.Rmd" target="_blank">R Notebook markdown for this page</a> 
       | <a href="https://raw.githubusercontent.com/Ratey-AtUWA/compositional_data/main/Hallberg.csv" target="_blank">Hallberg rock data</a> ]</p>
<hr width="50%" align="left">

<h1>6 Validation of LDA models for Supervised Classification</h1>

<p>We are using a curated version of a whole rock major element dataset
from Hallberg (<a
href="https://catalogue.data.wa.gov.au/dataset/hallberg-geochemistry"
class="uri" target="_blank">https://catalogue.data.wa.gov.au/dataset/hallberg-geochemistry</a>)</p>
<div id="use-alr-transformation-to-remove-closure"
class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> Use <em>ALR</em>
transformation to remove closure</h2>
<pre class="r"><code>Hallberg_alr &lt;- Hallberg
Hallberg_alr[,c(11:12,14:24)] &lt;- 
  alr(Hallberg_alr[,11:24], j = 3, ifwarn = FALSE) # recommend ifwarn = TRUE</code></pre>
<pre><code>##   The divisor is Al2O3</code></pre>
</div>
<div id="lda-on-closed-whole-rock-major-element-data"
class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> LDA on closed whole
rock major element data</h2>
<p>…using LDA to discriminate the Rock type – the predictors are the
major element oxide contents.</p>
<pre class="r"><code>data0 &lt;- Hallberg
data0[,c(11:24)] &lt;- scale(data0[,11:24]) # scale just numeric variables
lda_rock_clos &lt;- lda(formula = Rock ~ SiO2 + TiO2 + Fe2O3 + FeO + MnO + 
                       MgO + CaO + Na2O + K2O + P2O5,      # not Al2O3
                    data = data0,
                    prior = as.numeric(summary(Hallberg$Rock))/nrow(Hallberg))
cat(&quot;Proportions of between-categories variance explained by each LD\n&quot;)
props &lt;- matrix(lda_rock_clos$svd^2/sum(lda_rock_clos$svd^2),nrow = 1)
colnames(props) &lt;- paste0(&quot;LD&quot;,seq(1:length(props))); print(props, digits=3)</code></pre>
<pre><code>## Proportions of between-categories variance explained by each LD
##        LD1    LD2    LD3    LD4     LD5      LD6      LD7
## [1,] 0.868 0.0822 0.0325 0.0103 0.00599 0.000663 8.08e-05</code></pre>
</div>
<div id="lda-on-open-alr-whole-rock-major-element-data"
class="section level2" number="6.3">
<h2><span class="header-section-number">6.3</span> LDA on open (ALR)
whole rock major element data</h2>
<pre class="r"><code>data0 &lt;- Hallberg_alr
data0[,11:24] &lt;- scale(data0[,11:24]) # scale just numeric variables
lda_rock_open &lt;- lda(formula = Rock ~ SiO2 + TiO2 + Fe2O3 + FeO + MnO + 
                       MgO + CaO + Na2O + K2O + P2O5,       # not Al2O3
                    data = data0,
                    prior = as.numeric(summary(data0$Rock))/nrow(data0)) 
cat(&quot;Proportions of between-categories variance explained by each LD\n&quot;)
props &lt;- matrix(lda_rock_open$svd^2/sum(lda_rock_open$svd^2),nrow = 1)
colnames(props) &lt;- paste0(&quot;LD&quot;,seq(1:length(props))); print(props, digits=3)</code></pre>
<pre><code>## Proportions of between-categories variance explained by each LD
##        LD1    LD2    LD3    LD4     LD5     LD6      LD7
## [1,] 0.847 0.0934 0.0272 0.0218 0.00862 0.00221 0.000246</code></pre>
<p>We have restricted the normal output of LDA, as we’ve seen this in
previous sessions. As we’ve seen in those sessions, for the Hallberg
major element dataset, the first 2 LDA dimensions explain over 95% of
the between-categories variance.</p>
<p>We need to make objects containing the LDA predictions:</p>
<pre class="r"><code>ldaPred_rock_clos &lt;- predict(lda_rock_clos)
ldaPred_rock_open &lt;- predict(lda_rock_open)</code></pre>
</div>
<div
id="inspecting-the-agreement-between-actual-and-predicted-categories-in-lda"
class="section level2" number="6.4">
<h2><span class="header-section-number">6.4</span> Inspecting the
agreement between actual and predicted categories in LDA</h2>
<p>To do this easily we just make an R data frame with columns for the
actual categories (from the original data frame) and the predicted
categories (from the prediction objects we just made). We add a column
telling us if these two columns match in each row (which we can see
easily, but we use this column to calculate a numerical prediction
accuracy).</p>
<p>The code below uses the <em>head()</em> function to inspect the first
few rows of each comparison, but we could easily look at the whole
comparison data frames using <em>print()</em>.</p>
<pre class="r"><code>closComp &lt;- data.frame(Actual = as.character(Hallberg$Rock),
                       Predicted = as.character(ldaPred_rock_clos$class))
closComp$test &lt;- as.character(Hallberg_alr$Rock) == 
  as.character(ldaPred_rock_clos$class)
k = length(which(closComp$test == TRUE))
cat(&quot;Predictions by LDA using closed data:&quot;,k,&quot;out of&quot;,NROW(Hallberg_alr),
    &quot;=&quot;,paste0(round(100*k/NROW(Hallberg_alr),1),&quot;% correct\n&quot;))</code></pre>
<pre><code>## Predictions by LDA using closed data: 196 out of 321 = 61.1% correct</code></pre>
<pre class="r"><code>head(closComp, n = 10)</code></pre>
<pre><code>##    Actual    Predicted  test
## 1  Basalt       Basalt  TRUE
## 2  Basalt       Basalt  TRUE
## 3  Basalt       Basalt  TRUE
## 4  Basalt       Basalt  TRUE
## 5  Basalt       Basalt  TRUE
## 6  Basalt Metasediment FALSE
## 7  Basalt       Basalt  TRUE
## 8  Basalt Metasediment FALSE
## 9  Basalt       Basalt  TRUE
## 10 Basalt Metasediment FALSE</code></pre>
<pre class="r"><code>openComp &lt;- data.frame(Actual = as.character(Hallberg_alr$Rock),
                       Predicted = as.character(ldaPred_rock_open$class))
openComp$test &lt;- as.character(Hallberg_alr$Rock) == 
  as.character(ldaPred_rock_open$class)
k = length(which(openComp$test == TRUE))
cat(&quot;\nPredictions by LDA using open data:&quot;,k,&quot;out of&quot;,NROW(Hallberg_alr),
    &quot;=&quot;,paste0(round(100*k/NROW(Hallberg_alr),1),&quot;% correct\n&quot;))</code></pre>
<pre><code>## 
## Predictions by LDA using open data: 220 out of 321 = 68.5% correct</code></pre>
<pre class="r"><code>head(openComp, n = 10)</code></pre>
<pre><code>##    Actual    Predicted  test
## 1  Basalt     Dolerite FALSE
## 2  Basalt       Basalt  TRUE
## 3  Basalt       Basalt  TRUE
## 4  Basalt       Basalt  TRUE
## 5  Basalt       Basalt  TRUE
## 6  Basalt       Basalt  TRUE
## 7  Basalt       Basalt  TRUE
## 8  Basalt Metasediment FALSE
## 9  Basalt       Basalt  TRUE
## 10 Basalt Metasediment FALSE</code></pre>
<p>For this dataset, it seems as though LDA using either closed open
open data is not that good at predicting the Rock category for each
observation! In the output above, Basalt is mis-identified as Dolerite
(which might make sense given the expected compositional similarity;
simplistically, dolerite is a more coarse-grained version of basalt). It
seems to be more common for Basalt to be mis-identified as Metasediment,
which may or not make sense depending on the composition of the original
sediment!</p>
<p>This kind of comparison is not very rigorous, and nor does it address
the reason we might perform a supervised classification like LDA – to
use data to predict <em>unknown</em> categories. The ability of LDA to
predict unknown categories can be addressed by validation procedures,
such as the one we investigate below.</p>
</div>
<div
id="assessment-of-lda-prediction-using-a-training-validation-method"
class="section level2" number="6.5">
<h2><span class="header-section-number">6.5</span> Assessment of LDA
prediction using a training-validation method</h2>
<p>This can be done a different way, by including the <strong>CV =
TRUE</strong> option in the <strong>lda()</strong> function, which
implements ‘leave one out cross validation’. Simply stated, this omits
one observation at a time, running the LDA on the remaining data each
time, and predicting the probability of the missed observation being in
each category (the <em>posterior probabilities</em>). [Each observation
is assigned to the category with the greatest posterior
probability.]</p>
<p>We will use a related method, using ‘training’ and ‘validation’
subsets of our data. The idea here is that we divide the dataset into
two (the code below splits the data in half, but other splits could be
used, <em>e.g</em>. 0.75:0.25). The first subset of the data is used to
generate an LDA model (<em>i.e</em>. a set of linear discriminant
functions), which are then used to try and predict the categories in the
other subset. We choose the observations making up each subset randomly.
Of course, this could give us unreliable results if the random selection
happens to be somehow unbalanced, so we repeat the training-validation
process many times to calculate an average prediction rate.</p>
<p>One glitch that can happen is that in selecting a random subset of
the data, the subset may not include any samples from one or more
categories. This problem is more likely if our data have some categories
having relatively few observations. The code below applies an
‘error-catching’ condition before running the training LDA, which is
that all categories in a random subset need to be populated.</p>
<pre class="r"><code>n0 &lt;- 100 # number of iterations
ftrain &lt;- 0.5 # proportion of observations in training set
results &lt;- data.frame(
  Rep = rep(NA, n0),
  matches = rep(NA, n0),
  non_matches = rep(NA, n0),
  success = rep(NA, n0))
train &lt;- sample(1:NROW(Hallberg), round(NROW(Hallberg) * ftrain,0))
# make vector of individual category non-matches
matchByClass &lt;- 
  data.frame(Match1 = rep(0,nlevels(Hallberg$Rock[train]))) 
rownames(matchByClass) &lt;- levels(ldaPred_rock_clos$class)
fMatchXClass &lt;- 
  data.frame(PctMch1 = rep(0,nlevels(Hallberg$Rock[train]))) 
rownames(fMatchXClass) &lt;- levels(ldaPred_rock_clos$class)
# make vector of cumulative category counts in Hallberg[-train] iterations
cc0 &lt;- rep(0,nlevels(Hallberg$Rock))
isOK &lt;- 0 ; i &lt;- 2

for (i in 1:n0) {
  train &lt;- sample(1:NROW(Hallberg), round(NROW(Hallberg) * ftrain,0))
      # set condition requiring all categories to be populated
      if (is.na(match(NA,tapply(Hallberg[train,]$SiO2, 
                      Hallberg[train,]$Rock, sd, na.rm=T))) == TRUE) {
          lda_Rock_train &lt;- lda(formula = Rock ~ SiO2 + TiO2 + Fe2O3 + FeO + 
                            MnO + MgO + CaO + Na2O + K2O + P2O5, 
                          data = Hallberg[train,],
                          prior=as.numeric(summary(Hallberg$Rock[train]))/
                            nrow(Hallberg[train,]))
          ldaPred_rock_clos &lt;- predict(lda_Rock_train, Hallberg[-train,])
          isOK &lt;- isOK + 1
        }
  
  k=0               # number of matches
  m0 &lt;-             # vector of individual category matches 
    as.matrix(rep(0,nlevels(Hallberg$Rock[train]))) 
  rownames(m0) &lt;- levels(Hallberg$Rock)
  m1 &lt;-             # vector of fractional category matches 
    as.matrix(rep(0,nlevels(Hallberg$Rock[train]))) 
  rownames(m1) &lt;- levels(Hallberg$Rock)
  for (jM in 1:NROW(Hallberg[-train,])) {
    for (jS in 1:nlevels(ldaPred_rock_clos$class)) {
      if((ldaPred_rock_clos$class[jM] == levels(ldaPred_rock_clos$class)[jS]) &amp; 
         (Hallberg$Rock[-train][jM] == levels(ldaPred_rock_clos$class)[jS]) ) 
        m0[jS] = m0[jS] + 1
      else  m0[jS] = m0[jS] 
    }
    k = sum(m0)
  }
  cc0 &lt;- cc0 + as.numeric(summary(Hallberg$Rock[-train]))
  m1 &lt;- round(100*m0/as.numeric(summary(Hallberg$Rock[-train])),1)
  matchByClass[,paste0(&quot;Match&quot;,i)] &lt;- m0
  fMatchXClass[,paste0(&quot;PctMch&quot;,i)] &lt;- m1
  # output to results data frame: iteration, matches, non-matches, proportion matched
  results[i,] &lt;- c(i, k, NROW(Hallberg[-train,])-k, 
                   signif(k/NROW(Hallberg[-train,]),3))
}
# Output code block
cat(paste(&quot;[Based on&quot;, n0, &quot;random subsets of&quot;,paste0(100*ftrain,&quot;%&quot;),
          &quot;of the dataset to train LDA model\n&quot;,
     &quot;      to predict remaining observations]\n&quot;))
  cat(&quot;Number of obs. in random subsets =&quot;,NROW(train),
      &quot; (predicting&quot;,NROW(Hallberg)-NROW(train),&quot;samples)\n&quot;)
  print(numSummary(results[,2:4], statistics=c(&quot;mean&quot;,&quot;sd&quot;))$table)
  ns0 &lt;- numSummary(results$success)
  t0 &lt;- t.test(results$success)
  cat(rep(&quot;-\u2013-&quot;,24),
      &quot;\nStat. summary for &#39;success&#39;:\nMean = &quot;,round(ns0$table[1],4),
      &quot;, sd = &quot;,round(ns0$table[2],4),
      &quot;, 95% confidence interval = (&quot;,
      signif(t0$conf.int[1],3),&quot;, &quot;,signif(t0$conf.int[2],4),
      &quot;) (after &quot;,i,&quot; reps)\n&quot;, sep=&quot;&quot;)
  cat(n0-isOK,&quot;iterations &#39;failed&#39; due to randomisation missing a category\n\n&quot;)
  cat(&quot;Fraction of matches by category over ALL iterations:\n&quot;)
  summCats &lt;- data.frame(
    Rock_Type = row.names(matchByClass),
    Total_Matched = rowSums(matchByClass),
    Actual = cc0,
    Percent_Matched = paste0(round(100*(rowSums(matchByClass)/cc0),1),&quot;%&quot;),
    row.names = NULL)
  print(summCats)
# tidy up
rm(list = c(&quot;n0&quot;,&quot;ftrain&quot;,&quot;i&quot;,&quot;isOK&quot;,&quot;jS&quot;,&quot;jM&quot;,&quot;k&quot;,&quot;m0&quot;,&quot;m1&quot;,&quot;t0&quot;,&quot;cc0&quot;,
            &quot;matchByClass&quot;,&quot;fMatchXClass&quot;,&quot;results&quot;,&quot;train&quot;,&quot;summCats&quot;))</code></pre>
<pre><code>## [Based on 100 random subsets of 50% of the dataset to train LDA model
##        to predict remaining observations]
## Number of obs. in random subsets = 160  (predicting 161 samples)
##                  mean         sd
## matches     100.92000 6.58538326
## non_matches  60.08000 6.58538326
## success       0.62687 0.04092433
## -–--–--–--–--–--–--–--–--–--–--–--–--–--–--–--–--–--–--–--–--–--–--–--–-
## Stat. summary for &#39;success&#39;:
## Mean = 0.6269, sd = 0.0409, 95% confidence interval = (0.619, 0.635) (after 100 reps)
## 9 iterations &#39;failed&#39; due to randomisation missing a category
## 
## Fraction of matches by category over ALL iterations:
##           Rock_Type Total_Matched Actual Percent_Matched
## 1            Basalt          6350   7073           89.8%
## 2          Dolerite           339   2888           11.7%
## 3            Gabbro            66    977            6.8%
## 4    High Mg Basalt          1833   2515           72.9%
## 5      Metasediment           603    917           65.8%
## 6        Peridotite           558    896           62.3%
## 7        Pyroxenite           232    428           54.2%
## 8 Spinel Peridotite           111    406           27.3%</code></pre>
<p>The number of iterations makes some difference (Figure 6.1)!</p>
<p>So, it looks like we should run at least 50-100 iterations to get a
reasonable idea of how well our LDA model performs. More iterations is
better, but it depends how long you want the run-time to be!</p>
<p></p>
<div class="figure" style="text-align: center">
<img src="accuracy%20vs%20iterations-1.png" alt="Accuracy vs number of train-validate iterations" width="40%" />
<p class="caption">
Figure 6.1: Accuracy (shown by 95% confidence interval error bars) as a function of
number of train-validate iterations for the linear discriminant analysis
model using closed whole rock composition data.
</p>
</div>
<p>We can run a similar validation process for the ALR-transformed data
(we don’t show the code, as it’s effectively identical to the code for
validation of LDA for closed data, but with references to ‘Hallberg’
replaced with ‘Hallberg_alr’).</p>
<pre><code>## [Based on 100 random subsets of 50% of the dataset to train LDA model
##        to predict remaining observations]
## Number of obs. in random subsets = 160  (predicting 161 samples)
##                mean         sd
## matches     99.1000 5.38516481
## non_matches 61.9000 5.38516481
## success      0.6155 0.03350878
## -–--–--–--–--–--–--–--–--–--–--–--–--–--–--–--–--–--–--–--–--–--–--–--–-
## Stat. summary for &#39;success&#39;:
## Mean = 0.6155, sd = 0.0335, 95% confidence interval = (0.609, 0.6221) (after 100 reps)
## 6 iterations &#39;failed&#39; due to randomisation missing a category
## 
## Fraction of matches by category over ALL iterations:
##           Rock_Type Total_Matched Actual Percent_Matched
## 1            Basalt          6172   7078           87.2%
## 2          Dolerite           466   2996           15.6%
## 3            Gabbro            82    962            8.5%
## 4    High Mg Basalt          1877   2485           75.5%
## 5      Metasediment           611    902           67.7%
## 6        Peridotite           444    900           49.3%
## 7        Pyroxenite           219    370           59.2%
## 8 Spinel Peridotite            39    407            9.6%</code></pre>
<p>If we compare the validation output for the open data with the
results for closed data, we see very little difference in overall
accuracy. Both closed and open data generated successful predictions
62.7% of the time, with a slightly lower standard deviation for LDA
based on open data.</p>
<p>There were small differences (in this validation exercise), between
closed and open data, in the ability of LDA to predict specific
categories. Closed-data LDA seemed better at predicting Dolerite,
Gabbro, High Mg Basalt, and Metasediment. Conversely, LDA using open
data seemed better at predicting Basalt, Peridotite, Pyroxenite, and
Spinel Peridotite.</p>
<p>We may get better prediction accuracy by including different
variables in the LDA model. The original Hallberg geochemistry data at
<a href="https://catalogue.data.wa.gov.au/dataset/hallberg-geochemistry"
class="uri" target="_blank">https://catalogue.data.wa.gov.au/dataset/hallberg-geochemistry</a>
also contain trace element concentrations, which can sometimes provide
better discrimination than major element concentrations alone. This
would be an interesting exercise to extend one’s skills in data curation
and multivariate analysis of compositional data.</p>
<p>An issue that we haven’t considered yet is whether all the variables
we used for prediction are necessary. The R package
‘<strong>klaR</strong>’ (Weihs et al. 2005) includes the
<em>stepclass()</em> function, which enables us to refine an LDA model,
using a procedure similar to stepwise selection of predictors in
multiple regression.</p>

<hr width="50%" align="left">
       <p>[ <a href="compositional_Pt5-Hallberg_ALR-ILR_LDA.html">Previous page</a> | <a href="https://ratey-atuwa.github.io/pilot/index.html">Return to Contents page</a> |
       <a href="Pt7-usgs_LDA-on-PCA.html"> Next page</a> ]<br>
       [ <a href="https://raw.githubusercontent.com/Ratey-AtUWA/compositional_data/main/compositional_Pt6-LDA%20validation.Rmd" target="_blank">R Notebook markdown for this page</a> 
       | <a href="https://raw.githubusercontent.com/Ratey-AtUWA/compositional_data/main/Hallberg.csv" target="_blank">Hallberg rock data</a> ]</p>
<hr width="50%" align="left">

<div id="references" class="section level3" number="6.6">
<h2><span class="header-section-number">6.6</span> References</h2>
<p>Campbell, G. P., Curran, J. M., Miskelly, G. M., Coulson, S., Yaxley,
G. M., Grunsky, E. C., &amp; Simon C. Cox. (2009). Compositional data
analysis for elemental data in forensic science. <em>Forensic Science
International</em>, <strong>188</strong>, 81-90. <a
href="https://doi.org/10.1016/j.forsciint.2009.03.018"
class="uri" target="_blank">https://doi.org/10.1016/j.forsciint.2009.03.018</a></p>
<p>Fox, J. (2022). <em>RcmdrMisc: R Commander Miscellaneous
Functions</em>. R package version 2.7-2. <a
href="https://CRAN.R-project.org/package=RcmdrMisc"
class="uri" target="_blank">https://CRAN.R-project.org/package=RcmdrMisc</a></p>
<p>Fox, John and Sanford Weisberg (2019). <em>An {R} Companion to
Applied Regression</em> (<strong>car</strong>), Third Edition. Thousand
Oaks CA: Sage. URL: <a
href="https://socialsciences.mcmaster.ca/jfox/Books/Companion/"
class="uri" target="_blank">https://socialsciences.mcmaster.ca/jfox/Books/Companion/</a></p>
<p>Garrett, R.G. (2018). <em>rgr: Applied Geochemistry EDA</em>. R
package version 1.1.15. <a href="https://CRAN.R-project.org/package=rgr"
class="uri" target="_blank">https://CRAN.R-project.org/package=rgr</a></p>
<p>Grunsky, E. C. (2010). The interpretation of geochemical survey data.
<em>Geochemistry: Exploration, Environment, Analysis</em>,
<strong>10</strong>, 27-74. <a
href="https://doi.org/10.1144/1467-7873/09-210"
class="uri" target="_blank">https://doi.org/10.1144/1467-7873/09-210</a></p>
<p>Kassambara, A. and Mundt, F. (2020). <em>factoextra: Extract and
Visualize the Results of Multivariate Data Analyses</em>. R package
version 1.0.7. <a href="https://CRAN.R-project.org/package=factoextra"
class="uri" target="_blank">https://CRAN.R-project.org/package=factoextra</a></p>
<p>Maechler, M., Rousseeuw, P., Struyf, A., Hubert, M., Hornik,
K.(2021). <em>cluster: Cluster Analysis Basics and Extensions</em>. R
package version 2.1.2. <a
href="https://CRAN.R-project.org/package=cluster"
class="uri" target="_blank">https://CRAN.R-project.org/package=cluster</a></p>
<p>Reimann, C., Filzmoser, P., Garrett, R. G., and Dutter, R. (2008).
<em>Statistical Data Analysis Explained: Applied Environmental
Statistics with R</em> (First ed.). John Wiley &amp; Sons, Chichester,
UK.</p>
<p>Venables, W. N. and Ripley, B. D. (2002) <em>Modern Applied
Statistics with S</em> (<strong>MASS</strong>). Fourth Edition.
Springer, New York. ISBN 0-387-95457-0. <a
href="http://www.stats.ox.ac.uk/pub/MASS4/"
class="uri" target="_blank">http://www.stats.ox.ac.uk/pub/MASS4/</a></p>
<p>Weihs, C., Ligges, U., Luebke, K. and Raabe, N. (2005).
<strong>klaR</strong> – Analyzing German Business Cycles.
<strong>In</strong> Baier, D., Decker, R. and Schmidt-Thieme, L. (eds.).
<em>Data Analysis and Decision Support</em>, 335-343, Springer-Verlag,
Berlin.</p>
<p>Wickham, H. (2019). <em>stringr: Simple, Consistent Wrappers for
Common String Operations</em>. R package version 1.4.0. <a
href="https://CRAN.R-project.org/package=stringr"
class="uri" target="_blank">https://CRAN.R-project.org/package=stringr</a></p>
<p>Xu, N., Rate, A. W., &amp; Morgan, B. (2018). From source to sink:
Rare-earth elements trace the legacy of sulfuric dredge spoils on
estuarine sediments. <em>Science of The Total Environment</em>,
<strong>637-638</strong>, 1537-1549. <a
href="https://doi.org/10.1016/j.scitotenv.2018.04.398"
class="uri" target="_blank">https://doi.org/10.1016/j.scitotenv.2018.04.398</a></p>
</div>

<hr width="50%" align="left">
       <p>[ <a href="compositional_Pt5-Hallberg_ALR-ILR_LDA.html">Previous page</a> | <a href="https://ratey-atuwa.github.io/pilot/index.html">Return to Contents page</a> |
       <a href="Pt7-usgs_LDA-on-PCA.html"> Next page</a> ]<br>
       [ <a href="https://raw.githubusercontent.com/Ratey-AtUWA/compositional_data/main/compositional_Pt6-LDA%20validation.Rmd" target="_blank">R Notebook markdown for this page</a> 
       | <a href="https://raw.githubusercontent.com/Ratey-AtUWA/compositional_data/main/Hallberg.csv" target="_blank">Hallberg rock data</a> ]</p>
<hr width="50%" align="left">

</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
